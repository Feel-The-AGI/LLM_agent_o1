Good now i want us to do this
add a memory buffer and add that as context to the LLM with the User, system and agent conversation as well as a separate buffer for all previous thought processes it had
all these should be perfectly indexed
google gemini allows context so you can use that out of the box, i believe the curent model we are using is 10 million tokens.
optimize it for efficiency
try caching it using milvus: @https://milvus.io/docs in a vector data speed retrieval
Including the GPU functionallity
please please pelase dont delete anything thats important take your time
